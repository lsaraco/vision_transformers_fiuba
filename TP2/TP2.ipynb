{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT)\n",
    "\n",
    "El Vision Transformer (ViT), propuesto por Dosovitskiy et al. (2020), utiliza una arquitectura basada en Transformers para tareas de visión por computadora. A diferencia de las CNN, el ViT aplica **Multi-Head Self-Attention (MHSA)** para modelar relaciones globales entre partes de la imagen.\n",
    "\n",
    "## Arquitectura\n",
    "\n",
    "El ViT divide las imágenes en **patches** y las convierte en una secuencia de vectores de características (embeddings), que luego se procesan mediante capas Transformer. A continuación, se aplica el mecanismo de self-attention para modelar las relaciones entre estos patches.\n",
    "\n",
    "- **Embeddings**: Cada parche se aplana y se proyecta en un espacio de mayor dimensión.\n",
    "- **Positional Encoding**: Se agrega un embebido posicional para cada parche, permitiendo que el modelo capture las posiciones espaciales.\n",
    "---\n",
    "## ¿Cómo funciona el Self-Attention en ViT?\n",
    "\n",
    "Cada uno de los parches se considera un token similar a las palabras en el procesamiento del lenguaje natural (NLP).\n",
    "En el mecanismo de Self-Attention, cada uno de estos tokens (parches) puede interactuar con los demás tokens. Para hacerlo, el modelo calcula tres vectores para cada token: \n",
    "\n",
    "- **Query:** Representa qué está buscando el token.\n",
    "\n",
    "- **Key:** Representa una descripción de los demás tokens.\n",
    "\n",
    "- **Value:** Es la información que tiene cada token y que podría ser relevante para otros tokens.\n",
    "\n",
    "Luego, se calculan las similitudes entre el *Query* de un token y el *Key* de todos los demás tokens, lo que genera una \"puntuación de atención\". Esta puntuación determina cuánta atención debe prestar un token a otros. El valor *Value* de cada token se pondera según esta puntuación, lo que permite que un token integre información de todo el resto de la imagen.\n",
    "\n",
    "Este mecanismo es lo que permite que cada token (parche) en una imagen entienda no solo lo que está ocurriendo en su área local, sino también pueda considerar qué está sucediendo en otras partes de la imagen. \n",
    "\n",
    "La fórmula utilizada para calcular la atención es:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- **$Q$ (queries)**, **$K$ (keys)**, y **$V$ (values)** son las proyecciones lineales de los embeddings de los patches.\n",
    "- **$d_k$** es la dimensionalidad de las proyecciones, usada para escalar el producto punto y estabilizar el entrenamiento.\n",
    "\n",
    "Este proceso se aplica en múltiples cabezas, lo que permite al modelo aprender distintas representaciones.\n",
    "\n",
    "El mecanismo de self-attention utiliza las proyecciones:\n",
    "\n",
    "$$Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V$$\n",
    "\n",
    "Donde $X$ es el input y $W_Q$, $W_K$, y $W_V$ son matrices de pesos aprendidos.\n",
    "\n",
    "Después de aplicar el mecanismo de Self-Attention, los tokens se actualizan, ya que ahora tienen información de los demás tokens. Este proceso se repite varias veces en distintas capas de atención, profundizando la interacción entre los parches.\n",
    "\n",
    "Salida Final: Después de aplicar múltiples capas de Self-Attention, el modelo agrupa los tokens procesados para predecir la clase de la imagen o realizar otras tareas de visión.\n",
    "\n",
    "\n",
    "![Vision Transformer](vit_gif.gif)\n",
    "\n",
    "*Crédito: [lucidrains](https://github.com/lucidrains/vit-pytorch)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tareas:\n",
    "\n",
    "1. **Implementar la arquitectura de un Vision Transformer** \n",
    "\n",
    "2. **Ingresar y ajustar los parametros del modelo***\n",
    "\n",
    "3. **Probar diferentes técnicas de data augmentation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo:  cuda\n",
      "Torch version:  2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "from trainer import Trainer\n",
    "\n",
    "\n",
    "\n",
    "device =  'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print('Dispositivo: ',device)\n",
    "print('Torch version: ',torch.__version__)\n",
    "\n",
    "# La configuración, carga y preprocesamiento\n",
    "class ConfigPreprocess:\n",
    "    def __init__(self, device: str, img_path: str, img_size: int, patch_size: int):\n",
    "        self.device = device\n",
    "        self.img_path = img_path\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.test_img = self.load_image()\n",
    "\n",
    "    def load_image(self):\n",
    "        return TF.to_tensor(Image.open(self.img_path).resize((self.img_size, self.img_size))).unsqueeze(0).to(self.device)\n",
    "\n",
    "    def extract_patches(self, image: Tensor) -> Tensor:\n",
    "        patches = image.unfold(1, self.patch_size, self.patch_size).unfold(2, self.patch_size, self.patch_size)\n",
    "        patches = patches.contiguous().view(image.shape[0], -1, self.patch_size, self.patch_size)\n",
    "        return patches\n",
    "    \n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size: int, patch_size: int, in_channels: int = 3, embed_dim: int = 8):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # (B, embed_dim, H/patch_size, W/patch_size)\n",
    "        x = x.flatten(2)  # (B, embed_dim, num_patches)\n",
    "        x = x.transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class PositionalEncodingLearned(nn.Module):\n",
    "    \"\"\"Codigo que proviene del TP1.\"\"\"\n",
    "    def __init__(self, num_patches, embed_dim):\n",
    "        super(PositionalEncodingLearned, self).__init__()\n",
    "        # Aqui no uso register_buffer porque quiero que justamente este parámetro sea entrenable\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embedding\n",
    "\n",
    "\n",
    "\n",
    "# Parámetros\n",
    "img_path = \"raccoon.jpg\"\n",
    "img_size = 900\n",
    "patch_size = 64\n",
    "embed_dim = 8\n",
    "patch_idx = 0  # El índice del parche para el cual queres visualiizar la codificación posicional\n",
    "\n",
    "# Preprocesamiento\n",
    "config = ConfigPreprocess(device,img_path, img_size, patch_size)\n",
    "\n",
    "# Extracción de parches y visualización\n",
    "patches = config.extract_patches(config.test_img.squeeze(0))\n",
    "\n",
    "# Generación de embeddings\n",
    "embedded_patches = PatchEmbedding(img_size, patch_size, 3, embed_dim).to(config.device)\n",
    "patches = embedded_patches(config.test_img)\n",
    "\n",
    "# Codificación posicional\n",
    "\n",
    "num_patches = (img_size // patch_size) ** 2\n",
    "try:\n",
    "    positional_encoding = PositionalEncodingLearned(num_patches, embed_dim).to(config.device)\n",
    "    pos_embeddings = positional_encoding(patches)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Revise la existencia de la función PositionalEncodingLearned. Se produjo error durante la compilación: \\n {e}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, num_layers: int, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout,bias= False)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.transformer_encoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer\n",
    "Model\n",
    "> \n",
    "```ViT-Base ViT-Large ViT-Huge\n",
    "    Layers Hidden size D 12 768\n",
    "    24 1024 32 1280\n",
    "    MLP size Heads\n",
    "    Params\n",
    "    3072 12 86M 4096 16 307M 5120 16 632M\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Estructura del ViT.\"\"\"\n",
    "\n",
    "    def __init__(self, img_size, patch_size, embed_dim, num_heads, ff_dim, num_layers, num_classes=10, dropout=0.1,batch_first=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding de los patches\n",
    "        self.patch_embedding = PatchEmbedding(img_size, patch_size, 3, embed_dim) #3 porque son siempre imagenes RGB\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Token [CLS] que será el vector de clasificación\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "\n",
    "        # Codificación posicional, incluyendo el token [CLS]\n",
    "        self.positional_encoding = PositionalEncodingLearned(num_patches + 1, embed_dim)\n",
    "\n",
    "        # Capa de Transformer Encoder que proviene de la celda anterior\n",
    "        self.transformer_encoder = TransformerEncoder(embed_dim, num_heads, ff_dim, num_layers, dropout=dropout)\n",
    "\n",
    "        # MLP para la clasificación final\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),  # Normalización LayerNorm\n",
    "            nn.Linear(embed_dim, num_classes),  # Capa final para clasificar\n",
    "            nn.Dropout(dropout)  # Dropout antes de la salida\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]  # Tamaño del batch\n",
    "\n",
    "        # Obtener los embeddings de los patches\n",
    "        patches = self.patch_embedding(x)  # (B, num_patches, embed_dim)\n",
    "\n",
    "        # Expandir el token [CLS] para cada imagen en el batch\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n",
    "\n",
    "        # Concatenar el token [CLS] con los patches\n",
    "        x = torch.cat((cls_tokens, patches), dim=1)  # (B, 1 + num_patches, embed_dim)\n",
    "\n",
    "        # Aplicar la codificación posicional\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Pasar por el Transformer Encoder\n",
    "        x = self.transformer_encoder(x)  # (B, 1 + num_patches, embed_dim)\n",
    "\n",
    "        # Extraer la salida del token [CLS] (primera posición)\n",
    "        cls_output = x[:, 0]  # (B, embed_dim)\n",
    "\n",
    "        # Pasar por el MLP para obtener la predicción final\n",
    "        out = self.mlp_head(cls_output)  # (B, num_classes)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:09<00:00, 17.3MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define transformations for the input data\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784]),\n",
    "    #transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load your dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de carpeta: 340.19 MB\n"
     ]
    }
   ],
   "source": [
    "def get_folder_size(folder_path :os.PathLike) -> str:\n",
    "    total_size = 0\n",
    "    for dirpath, _, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "\n",
    "            if not os.path.islink(file_path):\n",
    "                total_size += os.path.getsize(file_path)\n",
    "\n",
    "    if total_size == 0:\n",
    "        return \"0B\"\n",
    "    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\")\n",
    "    i = int(math.floor(math.log(total_size, 1024)))\n",
    "    p = math.pow(1024, i)\n",
    "    s = round(total_size / p, 2)\n",
    "    return f\"{s} {size_name[i]}\"\n",
    "\n",
    "\n",
    "folder_path = './data'\n",
    "size_in_bytes = get_folder_size(folder_path)\n",
    "print(f\"Tamaño de carpeta: {size_in_bytes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(32//4)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parametros del modelo: [Parameter containing:\n",
      "tensor([[[-0.3098, -0.0606, -0.9974, -0.1758, -0.4217,  0.1747, -1.3449,\n",
      "          -1.0197,  0.2147,  1.1757,  1.5315, -0.9183, -0.4704, -0.5629,\n",
      "          -0.9023, -0.5371,  0.7896, -0.9198,  0.3980,  0.7474,  1.0054,\n",
      "          -0.8211, -0.8546,  0.6616,  1.4278, -0.6564,  1.3342,  1.4985,\n",
      "          -1.4912,  1.4381,  1.9858,  0.2181,  0.5167,  0.9205, -0.7222,\n",
      "           1.2771, -0.5846, -0.2833, -0.2278, -1.3692,  0.3701,  0.1570,\n",
      "          -0.0456, -1.1760, -0.4440,  0.5861, -1.3218,  1.0079, -0.9483,\n",
      "          -1.2200,  0.5776, -0.1939,  1.3602,  0.6680,  0.5109,  0.1252,\n",
      "          -0.5860, -1.4645,  1.5861,  0.7504,  0.5371, -0.0259,  0.5799,\n",
      "           1.3087]]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[[[-3.2738e-02, -8.2896e-02, -3.0997e-02, -4.0481e-02],\n",
      "          [ 1.0302e-01,  1.0996e-01,  1.2770e-01, -1.0088e-01],\n",
      "          [-1.3446e-01,  1.4213e-01,  6.9834e-02, -8.3685e-02],\n",
      "          [-1.3900e-01,  7.5729e-02, -9.3241e-02, -1.3873e-01]],\n",
      "\n",
      "         [[-2.2795e-03, -1.3685e-01,  5.7257e-02, -1.0721e-01],\n",
      "          [ 2.6913e-02,  8.3271e-03,  1.1392e-01,  3.9749e-02],\n",
      "          [ 5.2027e-02, -1.3047e-01, -1.8192e-02,  3.5968e-03],\n",
      "          [ 4.3503e-02,  3.5828e-02,  5.8775e-02,  1.4320e-01]],\n",
      "\n",
      "         [[ 1.2960e-01, -1.3068e-01, -3.1820e-02, -5.9960e-02],\n",
      "          [ 1.3825e-01, -2.8504e-02, -5.5240e-02, -6.2498e-02],\n",
      "          [-2.3982e-04, -4.4501e-02,  2.5886e-02, -2.7616e-02],\n",
      "          [-5.2716e-02,  1.2298e-01,  8.1266e-05, -1.0256e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.4773e-02, -5.9699e-02,  1.4832e-02,  1.2169e-01],\n",
      "          [-1.2171e-01,  1.2020e-01,  1.0844e-01, -9.1112e-02],\n",
      "          [-1.3602e-01, -1.1974e-03,  1.0356e-01,  6.3572e-02],\n",
      "          [ 4.0986e-02, -8.4722e-02,  4.4647e-02,  8.7054e-02]],\n",
      "\n",
      "         [[-1.0578e-01, -5.1989e-02, -1.0986e-01, -1.0805e-01],\n",
      "          [ 1.4070e-01,  1.2100e-01,  1.3980e-01, -1.0332e-01],\n",
      "          [-2.5225e-03,  1.3467e-01, -6.0435e-02,  7.5054e-02],\n",
      "          [ 7.2752e-02,  6.4448e-02,  1.1549e-01,  1.1243e-01]],\n",
      "\n",
      "         [[-5.6801e-02, -1.2827e-01,  6.5703e-02,  1.3285e-01],\n",
      "          [ 1.1865e-01, -2.2070e-02,  3.3650e-02,  7.3712e-02],\n",
      "          [ 3.7079e-02,  9.8641e-02, -1.1472e-01, -1.0272e-01],\n",
      "          [ 1.0383e-01, -1.0734e-01,  3.8904e-02,  1.0174e-01]]],\n",
      "\n",
      "\n",
      "        [[[-6.1404e-02,  4.3760e-02,  1.3789e-01, -8.0936e-02],\n",
      "          [-2.6738e-02,  4.6288e-02,  1.3012e-01,  1.2651e-01],\n",
      "          [-1.1099e-01, -1.2117e-01, -5.2360e-02, -6.6427e-02],\n",
      "          [ 9.5434e-02,  9.6160e-02,  1.0808e-01, -1.3476e-01]],\n",
      "\n",
      "         [[-4.1774e-02, -6.9889e-02,  6.8138e-02, -1.1484e-01],\n",
      "          [-1.1515e-01, -1.3631e-01,  9.6858e-02,  6.1456e-02],\n",
      "          [ 1.3374e-02,  9.9372e-02, -1.3972e-01,  1.1148e-01],\n",
      "          [-1.0907e-01, -1.1536e-01,  1.0230e-01, -1.3143e-02]],\n",
      "\n",
      "         [[ 8.4705e-02,  1.2254e-01, -5.8660e-02, -9.9067e-02],\n",
      "          [-1.3315e-02, -8.7796e-02, -1.3166e-01, -3.1160e-02],\n",
      "          [-9.6008e-02,  6.8653e-02,  1.3528e-01,  1.3189e-01],\n",
      "          [ 3.5340e-02,  1.0437e-01,  8.7804e-02, -2.5333e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.4167e-01,  6.3552e-02, -1.3638e-01, -1.3835e-01],\n",
      "          [-8.2485e-02,  7.3713e-02,  1.1513e-01,  1.3324e-01],\n",
      "          [ 1.3394e-01,  1.0763e-01,  4.4541e-02,  1.1394e-01],\n",
      "          [ 1.2065e-01,  3.9861e-02, -6.3491e-02, -1.5947e-03]],\n",
      "\n",
      "         [[ 5.0257e-02, -7.4352e-02, -1.3476e-01,  8.9471e-02],\n",
      "          [-1.4797e-02, -8.8662e-02, -6.2478e-02, -3.3766e-03],\n",
      "          [-1.0383e-02,  2.8871e-02,  1.4216e-01,  8.8125e-02],\n",
      "          [ 9.6241e-02,  9.6592e-02, -3.5161e-02,  1.2852e-01]],\n",
      "\n",
      "         [[ 9.9572e-02, -2.3656e-02, -7.6930e-02, -1.3468e-01],\n",
      "          [ 4.1404e-02,  6.7815e-02, -1.3589e-01, -6.3019e-02],\n",
      "          [ 4.8111e-02,  2.9350e-02,  1.0421e-01, -6.1352e-03],\n",
      "          [-9.3894e-02, -1.2003e-01,  7.5331e-02, -1.3805e-02]]],\n",
      "\n",
      "\n",
      "        [[[-9.4473e-02,  9.6649e-02,  3.1630e-02,  5.7640e-02],\n",
      "          [ 9.1665e-02, -4.2051e-02,  5.6065e-02,  7.4034e-02],\n",
      "          [-2.7846e-02, -4.2511e-02,  3.1993e-03,  9.0036e-02],\n",
      "          [ 9.0511e-02,  6.3406e-02,  6.9796e-02,  4.8344e-02]],\n",
      "\n",
      "         [[ 8.8780e-02, -3.8207e-02, -8.4427e-03,  9.4036e-02],\n",
      "          [-3.5571e-02,  4.6045e-02,  8.7069e-03,  3.5873e-02],\n",
      "          [-1.0220e-01, -1.8307e-03,  8.2220e-02, -1.0332e-01],\n",
      "          [ 1.2168e-02, -1.2776e-01,  1.5614e-02,  8.5210e-02]],\n",
      "\n",
      "         [[ 9.7816e-02,  1.2319e-01, -7.9491e-02, -6.6458e-02],\n",
      "          [-7.6226e-02, -1.2262e-01,  5.3645e-02, -2.5962e-02],\n",
      "          [-1.1689e-03,  9.1109e-02,  3.9205e-02, -5.2531e-02],\n",
      "          [-1.3621e-01,  3.8335e-02,  8.1884e-02, -9.8505e-04]]],\n",
      "\n",
      "\n",
      "        [[[-4.3249e-02,  9.7413e-02,  5.3203e-02,  1.1764e-01],\n",
      "          [ 9.5797e-02,  2.4567e-02,  2.7253e-02,  2.9205e-02],\n",
      "          [-2.8494e-02, -1.0279e-01, -5.4902e-02,  2.9202e-02],\n",
      "          [-3.3465e-02,  7.1903e-02,  4.3561e-02,  1.4010e-03]],\n",
      "\n",
      "         [[ 4.4250e-02, -2.1059e-03,  3.6565e-02,  2.9795e-02],\n",
      "          [ 3.9393e-02, -1.8471e-02, -1.2001e-02, -1.2760e-01],\n",
      "          [-1.2355e-01, -3.4549e-02, -8.9043e-02,  6.4254e-02],\n",
      "          [ 1.2752e-02,  2.1166e-02,  1.3785e-01, -1.4273e-01]],\n",
      "\n",
      "         [[-5.6392e-02,  2.9021e-02,  5.9480e-02, -8.1672e-02],\n",
      "          [-6.8236e-03,  3.4051e-02, -4.5469e-02,  1.2976e-01],\n",
      "          [-3.8876e-02,  1.4949e-02,  1.4251e-01, -3.7951e-02],\n",
      "          [ 5.9864e-02,  1.0916e-01,  2.6176e-02,  1.2066e-01]]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-1.2653e-01, -2.0973e-02, -6.1158e-02,  3.3664e-02,  2.9332e-02,\n",
      "         1.1788e-01, -1.5562e-02,  7.1274e-02,  3.2160e-02,  1.3562e-01,\n",
      "         9.8332e-02,  8.3389e-02,  3.1274e-02,  8.1161e-02,  3.0926e-02,\n",
      "        -3.7615e-02,  8.4313e-03,  1.0031e-01, -9.3263e-02,  4.8125e-02,\n",
      "         2.9451e-02, -9.8959e-02, -1.0408e-01,  2.6618e-02, -9.9474e-02,\n",
      "         1.0352e-01, -4.4640e-02,  9.9538e-02, -9.3188e-02, -9.7740e-02,\n",
      "        -1.2300e-01, -1.1924e-02, -7.5877e-02,  1.1219e-01, -1.0413e-01,\n",
      "        -8.7347e-02, -1.1346e-01, -4.2636e-02,  8.2426e-02,  1.1348e-01,\n",
      "        -1.0768e-01,  1.3413e-01, -1.3157e-01, -1.2512e-01, -1.6629e-02,\n",
      "         1.2824e-01, -5.8956e-02, -6.4356e-02, -8.9917e-02,  1.2217e-01,\n",
      "         1.3008e-01,  5.7612e-02, -9.1822e-02, -1.1182e-01, -7.0753e-05,\n",
      "         8.7999e-02,  6.0088e-02, -7.9690e-02, -1.1415e-01,  4.1068e-02,\n",
      "         1.5515e-02,  1.0724e-01,  1.0309e-01, -1.3196e-02], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[[-0.0729, -0.1975,  0.7058,  ...,  0.3926,  0.4480,  0.6427],\n",
      "         [-0.5318, -0.4496,  0.2612,  ...,  0.8058,  0.1232, -0.3469],\n",
      "         [-1.8150, -1.6812,  0.3728,  ..., -2.5302,  0.5777, -0.6195],\n",
      "         ...,\n",
      "         [-0.2387, -0.1603, -0.1933,  ...,  1.0397, -0.2168, -1.2606],\n",
      "         [ 0.3333,  0.4084,  0.9983,  ...,  0.9348, -0.1831,  1.3987],\n",
      "         [ 1.7676,  0.6066, -1.6493,  ...,  0.3710, -1.1551,  0.2836]]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0257,  0.0912,  0.0294,  ...,  0.0582,  0.0309,  0.0599],\n",
      "        [ 0.1145,  0.1158, -0.0456,  ...,  0.0844,  0.0846, -0.0594],\n",
      "        [-0.0114,  0.0525,  0.0783,  ..., -0.1502, -0.1434,  0.1251],\n",
      "        ...,\n",
      "        [ 0.1225, -0.1424, -0.1511,  ...,  0.1171, -0.1494,  0.1459],\n",
      "        [ 0.1520,  0.1021,  0.0043,  ...,  0.1112, -0.0607, -0.0801],\n",
      "        [-0.0811,  0.1218,  0.0550,  ...,  0.0650,  0.1123, -0.0108]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0292, -0.1084, -0.1183,  ..., -0.0800, -0.0801, -0.0015],\n",
      "        [-0.0460, -0.1121,  0.0904,  ..., -0.0116,  0.0033,  0.0314],\n",
      "        [ 0.0170, -0.0791,  0.0508,  ...,  0.0881,  0.0083, -0.0172],\n",
      "        ...,\n",
      "        [-0.0116, -0.0936, -0.0428,  ...,  0.0042, -0.1019, -0.0359],\n",
      "        [ 0.0609,  0.0246,  0.0234,  ...,  0.0685,  0.1190, -0.0422],\n",
      "        [ 0.0518, -0.0584, -0.0548,  ...,  0.0455, -0.0396,  0.0893]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-7.7753e-02,  6.9562e-02, -4.5382e-02, -8.3752e-02, -1.8964e-02,\n",
      "         -5.7664e-02, -8.7453e-02,  1.2315e-01, -1.0093e-01, -1.1933e-01,\n",
      "         -8.9562e-02,  6.8992e-02, -1.2074e-02, -1.2168e-01, -8.0799e-02,\n",
      "          5.9548e-02,  8.5885e-02, -9.4927e-03, -6.1447e-02,  8.9896e-02,\n",
      "         -1.0368e-01, -6.4434e-02, -3.2492e-02, -5.7742e-02, -3.1359e-02,\n",
      "         -1.2410e-01,  7.5308e-02,  1.2008e-01, -7.2129e-02, -5.4292e-02,\n",
      "          5.7883e-02, -9.5622e-02, -3.3085e-02,  1.1867e-01,  1.0234e-01,\n",
      "         -3.6504e-02, -5.1112e-02,  3.9874e-03, -8.2113e-02,  3.3006e-02,\n",
      "         -1.1222e-01, -6.0341e-02, -1.1557e-02, -6.3293e-02,  9.1044e-02,\n",
      "          4.1406e-02, -6.3171e-02,  6.4813e-02,  1.2361e-01,  3.9943e-02,\n",
      "         -6.6733e-02, -1.5993e-02,  8.6844e-02, -1.3379e-03, -6.1584e-02,\n",
      "         -9.5931e-02,  7.2285e-02, -1.2255e-01,  1.2394e-01, -3.0802e-02,\n",
      "         -2.7434e-02, -5.9959e-02, -1.5993e-02,  3.2512e-02],\n",
      "        [-1.2019e-01,  2.7644e-02, -9.0839e-02,  4.7231e-03, -5.0495e-03,\n",
      "         -3.9608e-02, -5.6483e-02,  7.8572e-02, -5.8354e-02, -2.9821e-02,\n",
      "          6.0005e-02, -9.9926e-02,  6.3625e-02, -1.1873e-01,  4.9762e-02,\n",
      "          6.9814e-02,  4.6325e-02, -3.5424e-02, -9.0752e-02,  1.0264e-01,\n",
      "          7.6255e-02, -6.6729e-03,  1.0969e-01, -9.5121e-02, -1.1247e-04,\n",
      "         -4.9915e-02, -1.0217e-01, -5.4319e-02,  3.7232e-02, -4.7894e-02,\n",
      "          7.6289e-02,  7.8802e-03, -8.5268e-03, -4.1980e-02,  5.5268e-02,\n",
      "          1.1364e-01,  5.0954e-02,  2.4183e-02,  4.9942e-02, -1.2066e-01,\n",
      "         -1.1487e-02,  2.2614e-02, -1.0825e-01,  6.2411e-02,  4.0344e-02,\n",
      "         -1.0813e-01, -5.8878e-02, -5.7174e-02, -4.3368e-02,  1.0495e-02,\n",
      "          7.4747e-02,  7.9642e-02, -3.6204e-02,  6.9127e-02,  2.1014e-03,\n",
      "         -1.2457e-01, -2.2084e-02,  1.0508e-01,  9.1059e-02, -2.2687e-02,\n",
      "          2.7607e-02,  8.5458e-02, -1.1779e-01, -1.0568e-02]], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.3849, -0.6414],\n",
      "        [-0.4145, -0.3837],\n",
      "        [-0.0993,  0.2523],\n",
      "        [-0.7022,  0.2683],\n",
      "        [ 0.1251,  0.2969],\n",
      "        [ 0.4485,  0.5553],\n",
      "        [-0.4150, -0.2087],\n",
      "        [-0.6234, -0.2838],\n",
      "        [ 0.5744,  0.5514],\n",
      "        [-0.4644,  0.3680],\n",
      "        [-0.6210,  0.6414],\n",
      "        [ 0.2160,  0.3745],\n",
      "        [-0.0317, -0.1937],\n",
      "        [-0.0747,  0.4603],\n",
      "        [-0.5977, -0.5295],\n",
      "        [ 0.0041,  0.6085],\n",
      "        [ 0.3369,  0.1837],\n",
      "        [ 0.5613, -0.0803],\n",
      "        [ 0.4518, -0.2153],\n",
      "        [-0.3562,  0.4297],\n",
      "        [-0.0398,  0.3173],\n",
      "        [ 0.6315, -0.1564],\n",
      "        [ 0.7026,  0.4899],\n",
      "        [-0.6397, -0.5459],\n",
      "        [ 0.6811, -0.5364],\n",
      "        [ 0.3492,  0.2536],\n",
      "        [-0.3028,  0.2535],\n",
      "        [-0.3527,  0.0343],\n",
      "        [-0.7013, -0.4808],\n",
      "        [-0.6031, -0.6559],\n",
      "        [-0.0034, -0.3962],\n",
      "        [ 0.5781, -0.6810],\n",
      "        [ 0.2605, -0.5371],\n",
      "        [ 0.0564, -0.3667],\n",
      "        [-0.2293,  0.5012],\n",
      "        [-0.2965,  0.0311],\n",
      "        [-0.2964, -0.0327],\n",
      "        [-0.4880,  0.0170],\n",
      "        [ 0.5013, -0.0162],\n",
      "        [ 0.0782, -0.2303],\n",
      "        [-0.1392,  0.4030],\n",
      "        [-0.2435,  0.2744],\n",
      "        [-0.5628, -0.2933],\n",
      "        [ 0.5493,  0.6951],\n",
      "        [ 0.0151, -0.4014],\n",
      "        [-0.5026,  0.2618],\n",
      "        [ 0.1947,  0.7047],\n",
      "        [ 0.6010,  0.2551],\n",
      "        [-0.5554, -0.3094],\n",
      "        [-0.2765, -0.6627],\n",
      "        [-0.5972,  0.1285],\n",
      "        [-0.2279,  0.1440],\n",
      "        [-0.3000,  0.6706],\n",
      "        [ 0.7007,  0.4311],\n",
      "        [ 0.5011, -0.4491],\n",
      "        [-0.4014, -0.4072],\n",
      "        [ 0.6662, -0.2744],\n",
      "        [-0.0949, -0.1794],\n",
      "        [-0.0768, -0.2092],\n",
      "        [-0.4157,  0.2000],\n",
      "        [-0.4971,  0.0374],\n",
      "        [-0.0741,  0.4895],\n",
      "        [ 0.2880,  0.1260],\n",
      "        [ 0.4006, -0.0834]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0257,  0.0912,  0.0294,  ...,  0.0582,  0.0309,  0.0599],\n",
      "        [ 0.1145,  0.1158, -0.0456,  ...,  0.0844,  0.0846, -0.0594],\n",
      "        [-0.0114,  0.0525,  0.0783,  ..., -0.1502, -0.1434,  0.1251],\n",
      "        ...,\n",
      "        [ 0.1225, -0.1424, -0.1511,  ...,  0.1171, -0.1494,  0.1459],\n",
      "        [ 0.1520,  0.1021,  0.0043,  ...,  0.1112, -0.0607, -0.0801],\n",
      "        [-0.0811,  0.1218,  0.0550,  ...,  0.0650,  0.1123, -0.0108]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0292, -0.1084, -0.1183,  ..., -0.0800, -0.0801, -0.0015],\n",
      "        [-0.0460, -0.1121,  0.0904,  ..., -0.0116,  0.0033,  0.0314],\n",
      "        [ 0.0170, -0.0791,  0.0508,  ...,  0.0881,  0.0083, -0.0172],\n",
      "        ...,\n",
      "        [-0.0116, -0.0936, -0.0428,  ...,  0.0042, -0.1019, -0.0359],\n",
      "        [ 0.0609,  0.0246,  0.0234,  ...,  0.0685,  0.1190, -0.0422],\n",
      "        [ 0.0518, -0.0584, -0.0548,  ...,  0.0455, -0.0396,  0.0893]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-7.7753e-02,  6.9562e-02, -4.5382e-02, -8.3752e-02, -1.8964e-02,\n",
      "         -5.7664e-02, -8.7453e-02,  1.2315e-01, -1.0093e-01, -1.1933e-01,\n",
      "         -8.9562e-02,  6.8992e-02, -1.2074e-02, -1.2168e-01, -8.0799e-02,\n",
      "          5.9548e-02,  8.5885e-02, -9.4927e-03, -6.1447e-02,  8.9896e-02,\n",
      "         -1.0368e-01, -6.4434e-02, -3.2492e-02, -5.7742e-02, -3.1359e-02,\n",
      "         -1.2410e-01,  7.5308e-02,  1.2008e-01, -7.2129e-02, -5.4292e-02,\n",
      "          5.7883e-02, -9.5622e-02, -3.3085e-02,  1.1867e-01,  1.0234e-01,\n",
      "         -3.6504e-02, -5.1112e-02,  3.9874e-03, -8.2113e-02,  3.3006e-02,\n",
      "         -1.1222e-01, -6.0341e-02, -1.1557e-02, -6.3293e-02,  9.1044e-02,\n",
      "          4.1406e-02, -6.3171e-02,  6.4813e-02,  1.2361e-01,  3.9943e-02,\n",
      "         -6.6733e-02, -1.5993e-02,  8.6844e-02, -1.3379e-03, -6.1584e-02,\n",
      "         -9.5931e-02,  7.2285e-02, -1.2255e-01,  1.2394e-01, -3.0802e-02,\n",
      "         -2.7434e-02, -5.9959e-02, -1.5993e-02,  3.2512e-02],\n",
      "        [-1.2019e-01,  2.7644e-02, -9.0839e-02,  4.7231e-03, -5.0495e-03,\n",
      "         -3.9608e-02, -5.6483e-02,  7.8572e-02, -5.8354e-02, -2.9821e-02,\n",
      "          6.0005e-02, -9.9926e-02,  6.3625e-02, -1.1873e-01,  4.9762e-02,\n",
      "          6.9814e-02,  4.6325e-02, -3.5424e-02, -9.0752e-02,  1.0264e-01,\n",
      "          7.6255e-02, -6.6729e-03,  1.0969e-01, -9.5121e-02, -1.1247e-04,\n",
      "         -4.9915e-02, -1.0217e-01, -5.4319e-02,  3.7232e-02, -4.7894e-02,\n",
      "          7.6289e-02,  7.8802e-03, -8.5268e-03, -4.1980e-02,  5.5268e-02,\n",
      "          1.1364e-01,  5.0954e-02,  2.4183e-02,  4.9942e-02, -1.2066e-01,\n",
      "         -1.1487e-02,  2.2614e-02, -1.0825e-01,  6.2411e-02,  4.0344e-02,\n",
      "         -1.0813e-01, -5.8878e-02, -5.7174e-02, -4.3368e-02,  1.0495e-02,\n",
      "          7.4747e-02,  7.9642e-02, -3.6204e-02,  6.9127e-02,  2.1014e-03,\n",
      "         -1.2457e-01, -2.2084e-02,  1.0508e-01,  9.1059e-02, -2.2687e-02,\n",
      "          2.7607e-02,  8.5458e-02, -1.1779e-01, -1.0568e-02]], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.3849, -0.6414],\n",
      "        [-0.4145, -0.3837],\n",
      "        [-0.0993,  0.2523],\n",
      "        [-0.7022,  0.2683],\n",
      "        [ 0.1251,  0.2969],\n",
      "        [ 0.4485,  0.5553],\n",
      "        [-0.4150, -0.2087],\n",
      "        [-0.6234, -0.2838],\n",
      "        [ 0.5744,  0.5514],\n",
      "        [-0.4644,  0.3680],\n",
      "        [-0.6210,  0.6414],\n",
      "        [ 0.2160,  0.3745],\n",
      "        [-0.0317, -0.1937],\n",
      "        [-0.0747,  0.4603],\n",
      "        [-0.5977, -0.5295],\n",
      "        [ 0.0041,  0.6085],\n",
      "        [ 0.3369,  0.1837],\n",
      "        [ 0.5613, -0.0803],\n",
      "        [ 0.4518, -0.2153],\n",
      "        [-0.3562,  0.4297],\n",
      "        [-0.0398,  0.3173],\n",
      "        [ 0.6315, -0.1564],\n",
      "        [ 0.7026,  0.4899],\n",
      "        [-0.6397, -0.5459],\n",
      "        [ 0.6811, -0.5364],\n",
      "        [ 0.3492,  0.2536],\n",
      "        [-0.3028,  0.2535],\n",
      "        [-0.3527,  0.0343],\n",
      "        [-0.7013, -0.4808],\n",
      "        [-0.6031, -0.6559],\n",
      "        [-0.0034, -0.3962],\n",
      "        [ 0.5781, -0.6810],\n",
      "        [ 0.2605, -0.5371],\n",
      "        [ 0.0564, -0.3667],\n",
      "        [-0.2293,  0.5012],\n",
      "        [-0.2965,  0.0311],\n",
      "        [-0.2964, -0.0327],\n",
      "        [-0.4880,  0.0170],\n",
      "        [ 0.5013, -0.0162],\n",
      "        [ 0.0782, -0.2303],\n",
      "        [-0.1392,  0.4030],\n",
      "        [-0.2435,  0.2744],\n",
      "        [-0.5628, -0.2933],\n",
      "        [ 0.5493,  0.6951],\n",
      "        [ 0.0151, -0.4014],\n",
      "        [-0.5026,  0.2618],\n",
      "        [ 0.1947,  0.7047],\n",
      "        [ 0.6010,  0.2551],\n",
      "        [-0.5554, -0.3094],\n",
      "        [-0.2765, -0.6627],\n",
      "        [-0.5972,  0.1285],\n",
      "        [-0.2279,  0.1440],\n",
      "        [-0.3000,  0.6706],\n",
      "        [ 0.7007,  0.4311],\n",
      "        [ 0.5011, -0.4491],\n",
      "        [-0.4014, -0.4072],\n",
      "        [ 0.6662, -0.2744],\n",
      "        [-0.0949, -0.1794],\n",
      "        [-0.0768, -0.2092],\n",
      "        [-0.4157,  0.2000],\n",
      "        [-0.4971,  0.0374],\n",
      "        [-0.0741,  0.4895],\n",
      "        [ 0.2880,  0.1260],\n",
      "        [ 0.4006, -0.0834]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0257,  0.0912,  0.0294,  ...,  0.0582,  0.0309,  0.0599],\n",
      "        [ 0.1145,  0.1158, -0.0456,  ...,  0.0844,  0.0846, -0.0594],\n",
      "        [-0.0114,  0.0525,  0.0783,  ..., -0.1502, -0.1434,  0.1251],\n",
      "        ...,\n",
      "        [ 0.1225, -0.1424, -0.1511,  ...,  0.1171, -0.1494,  0.1459],\n",
      "        [ 0.1520,  0.1021,  0.0043,  ...,  0.1112, -0.0607, -0.0801],\n",
      "        [-0.0811,  0.1218,  0.0550,  ...,  0.0650,  0.1123, -0.0108]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0292, -0.1084, -0.1183,  ..., -0.0800, -0.0801, -0.0015],\n",
      "        [-0.0460, -0.1121,  0.0904,  ..., -0.0116,  0.0033,  0.0314],\n",
      "        [ 0.0170, -0.0791,  0.0508,  ...,  0.0881,  0.0083, -0.0172],\n",
      "        ...,\n",
      "        [-0.0116, -0.0936, -0.0428,  ...,  0.0042, -0.1019, -0.0359],\n",
      "        [ 0.0609,  0.0246,  0.0234,  ...,  0.0685,  0.1190, -0.0422],\n",
      "        [ 0.0518, -0.0584, -0.0548,  ...,  0.0455, -0.0396,  0.0893]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[-7.7753e-02,  6.9562e-02, -4.5382e-02, -8.3752e-02, -1.8964e-02,\n",
      "         -5.7664e-02, -8.7453e-02,  1.2315e-01, -1.0093e-01, -1.1933e-01,\n",
      "         -8.9562e-02,  6.8992e-02, -1.2074e-02, -1.2168e-01, -8.0799e-02,\n",
      "          5.9548e-02,  8.5885e-02, -9.4927e-03, -6.1447e-02,  8.9896e-02,\n",
      "         -1.0368e-01, -6.4434e-02, -3.2492e-02, -5.7742e-02, -3.1359e-02,\n",
      "         -1.2410e-01,  7.5308e-02,  1.2008e-01, -7.2129e-02, -5.4292e-02,\n",
      "          5.7883e-02, -9.5622e-02, -3.3085e-02,  1.1867e-01,  1.0234e-01,\n",
      "         -3.6504e-02, -5.1112e-02,  3.9874e-03, -8.2113e-02,  3.3006e-02,\n",
      "         -1.1222e-01, -6.0341e-02, -1.1557e-02, -6.3293e-02,  9.1044e-02,\n",
      "          4.1406e-02, -6.3171e-02,  6.4813e-02,  1.2361e-01,  3.9943e-02,\n",
      "         -6.6733e-02, -1.5993e-02,  8.6844e-02, -1.3379e-03, -6.1584e-02,\n",
      "         -9.5931e-02,  7.2285e-02, -1.2255e-01,  1.2394e-01, -3.0802e-02,\n",
      "         -2.7434e-02, -5.9959e-02, -1.5993e-02,  3.2512e-02],\n",
      "        [-1.2019e-01,  2.7644e-02, -9.0839e-02,  4.7231e-03, -5.0495e-03,\n",
      "         -3.9608e-02, -5.6483e-02,  7.8572e-02, -5.8354e-02, -2.9821e-02,\n",
      "          6.0005e-02, -9.9926e-02,  6.3625e-02, -1.1873e-01,  4.9762e-02,\n",
      "          6.9814e-02,  4.6325e-02, -3.5424e-02, -9.0752e-02,  1.0264e-01,\n",
      "          7.6255e-02, -6.6729e-03,  1.0969e-01, -9.5121e-02, -1.1247e-04,\n",
      "         -4.9915e-02, -1.0217e-01, -5.4319e-02,  3.7232e-02, -4.7894e-02,\n",
      "          7.6289e-02,  7.8802e-03, -8.5268e-03, -4.1980e-02,  5.5268e-02,\n",
      "          1.1364e-01,  5.0954e-02,  2.4183e-02,  4.9942e-02, -1.2066e-01,\n",
      "         -1.1487e-02,  2.2614e-02, -1.0825e-01,  6.2411e-02,  4.0344e-02,\n",
      "         -1.0813e-01, -5.8878e-02, -5.7174e-02, -4.3368e-02,  1.0495e-02,\n",
      "          7.4747e-02,  7.9642e-02, -3.6204e-02,  6.9127e-02,  2.1014e-03,\n",
      "         -1.2457e-01, -2.2084e-02,  1.0508e-01,  9.1059e-02, -2.2687e-02,\n",
      "          2.7607e-02,  8.5458e-02, -1.1779e-01, -1.0568e-02]], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.3849, -0.6414],\n",
      "        [-0.4145, -0.3837],\n",
      "        [-0.0993,  0.2523],\n",
      "        [-0.7022,  0.2683],\n",
      "        [ 0.1251,  0.2969],\n",
      "        [ 0.4485,  0.5553],\n",
      "        [-0.4150, -0.2087],\n",
      "        [-0.6234, -0.2838],\n",
      "        [ 0.5744,  0.5514],\n",
      "        [-0.4644,  0.3680],\n",
      "        [-0.6210,  0.6414],\n",
      "        [ 0.2160,  0.3745],\n",
      "        [-0.0317, -0.1937],\n",
      "        [-0.0747,  0.4603],\n",
      "        [-0.5977, -0.5295],\n",
      "        [ 0.0041,  0.6085],\n",
      "        [ 0.3369,  0.1837],\n",
      "        [ 0.5613, -0.0803],\n",
      "        [ 0.4518, -0.2153],\n",
      "        [-0.3562,  0.4297],\n",
      "        [-0.0398,  0.3173],\n",
      "        [ 0.6315, -0.1564],\n",
      "        [ 0.7026,  0.4899],\n",
      "        [-0.6397, -0.5459],\n",
      "        [ 0.6811, -0.5364],\n",
      "        [ 0.3492,  0.2536],\n",
      "        [-0.3028,  0.2535],\n",
      "        [-0.3527,  0.0343],\n",
      "        [-0.7013, -0.4808],\n",
      "        [-0.6031, -0.6559],\n",
      "        [-0.0034, -0.3962],\n",
      "        [ 0.5781, -0.6810],\n",
      "        [ 0.2605, -0.5371],\n",
      "        [ 0.0564, -0.3667],\n",
      "        [-0.2293,  0.5012],\n",
      "        [-0.2965,  0.0311],\n",
      "        [-0.2964, -0.0327],\n",
      "        [-0.4880,  0.0170],\n",
      "        [ 0.5013, -0.0162],\n",
      "        [ 0.0782, -0.2303],\n",
      "        [-0.1392,  0.4030],\n",
      "        [-0.2435,  0.2744],\n",
      "        [-0.5628, -0.2933],\n",
      "        [ 0.5493,  0.6951],\n",
      "        [ 0.0151, -0.4014],\n",
      "        [-0.5026,  0.2618],\n",
      "        [ 0.1947,  0.7047],\n",
      "        [ 0.6010,  0.2551],\n",
      "        [-0.5554, -0.3094],\n",
      "        [-0.2765, -0.6627],\n",
      "        [-0.5972,  0.1285],\n",
      "        [-0.2279,  0.1440],\n",
      "        [-0.3000,  0.6706],\n",
      "        [ 0.7007,  0.4311],\n",
      "        [ 0.5011, -0.4491],\n",
      "        [-0.4014, -0.4072],\n",
      "        [ 0.6662, -0.2744],\n",
      "        [-0.0949, -0.1794],\n",
      "        [-0.0768, -0.2092],\n",
      "        [-0.4157,  0.2000],\n",
      "        [-0.4971,  0.0374],\n",
      "        [-0.0741,  0.4895],\n",
      "        [ 0.2880,  0.1260],\n",
      "        [ 0.4006, -0.0834]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0931, -0.0978, -0.1025, -0.0525,  0.0565, -0.0856, -0.1225, -0.0641,\n",
      "          0.0483,  0.0148,  0.1131, -0.0011, -0.1134, -0.0807,  0.0239,  0.1240,\n",
      "         -0.1023,  0.0112, -0.0206, -0.0460, -0.1124,  0.0156,  0.1042,  0.0083,\n",
      "          0.0587,  0.0158,  0.0547, -0.1169,  0.0527,  0.1237,  0.0786,  0.0499,\n",
      "          0.0780,  0.1056,  0.0970,  0.0794,  0.0326,  0.0618, -0.1213,  0.0486,\n",
      "          0.0612, -0.0239,  0.0372,  0.0040, -0.0132, -0.0997, -0.0463,  0.0913,\n",
      "         -0.0685, -0.0666, -0.1008, -0.0102,  0.0478, -0.0691, -0.0533,  0.0869,\n",
      "         -0.0310,  0.0626,  0.1229,  0.0156,  0.0615,  0.0096,  0.0549,  0.0194],\n",
      "        [-0.0999, -0.0442, -0.0036, -0.0810, -0.0707, -0.0651,  0.0885,  0.0556,\n",
      "          0.0412,  0.0167, -0.0035, -0.1063,  0.1193,  0.0956, -0.0249, -0.0619,\n",
      "         -0.0082, -0.0709, -0.1008,  0.0887,  0.0174,  0.0300, -0.0512, -0.0723,\n",
      "         -0.0089,  0.1188, -0.0086,  0.0461, -0.0802,  0.0502, -0.0797, -0.0564,\n",
      "         -0.0578, -0.0422, -0.0198,  0.0398, -0.0882,  0.1242, -0.0405, -0.0787,\n",
      "          0.1096, -0.0585,  0.0861, -0.1201, -0.0042,  0.1103, -0.0489,  0.0850,\n",
      "         -0.0677,  0.1151,  0.0029,  0.0271,  0.0517, -0.1159, -0.1111, -0.0563,\n",
      "         -0.1072, -0.0326,  0.0995,  0.0796,  0.0596,  0.1077, -0.0772, -0.0231],\n",
      "        [ 0.0396,  0.0811,  0.1055, -0.0041,  0.0254,  0.0232,  0.1107, -0.0355,\n",
      "         -0.0773, -0.0851,  0.0740,  0.0633,  0.1075,  0.0553, -0.0157,  0.0595,\n",
      "         -0.0059, -0.0754,  0.0045, -0.0356,  0.0363, -0.0895, -0.0712,  0.0734,\n",
      "         -0.0115,  0.0153, -0.0026,  0.0134, -0.0772,  0.0683,  0.0687,  0.1192,\n",
      "          0.1197, -0.0841, -0.0438, -0.0124,  0.0661,  0.0553, -0.0978,  0.0315,\n",
      "          0.0135, -0.0823,  0.0730, -0.0831,  0.0482, -0.0022, -0.0766, -0.0476,\n",
      "         -0.1183, -0.0205, -0.1190, -0.0515, -0.0331,  0.0699, -0.0991,  0.0722,\n",
      "         -0.0056,  0.0179, -0.0251, -0.0534, -0.0750, -0.0228,  0.1098, -0.1084],\n",
      "        [-0.0204, -0.0079, -0.0637,  0.0525, -0.0384, -0.0356, -0.1221,  0.0062,\n",
      "          0.0048, -0.0842,  0.1068, -0.0680,  0.0007,  0.0250,  0.1042, -0.0591,\n",
      "          0.0995, -0.1091,  0.0784,  0.0585,  0.0199, -0.0766,  0.0082, -0.0918,\n",
      "          0.0613, -0.0724,  0.0124, -0.0155, -0.0340, -0.0452, -0.0825, -0.0149,\n",
      "         -0.0297,  0.0528,  0.0686, -0.0717,  0.1000,  0.0070, -0.1025, -0.0287,\n",
      "         -0.1019,  0.0935, -0.1157,  0.0364,  0.0127, -0.0712,  0.0450, -0.0073,\n",
      "          0.0143,  0.0330,  0.0282,  0.0329, -0.0506, -0.0847,  0.1250,  0.0628,\n",
      "          0.0651,  0.0520, -0.0889, -0.0806, -0.0657, -0.0631, -0.0783,  0.0580],\n",
      "        [ 0.1090,  0.0647,  0.0112,  0.0511, -0.0630,  0.1076,  0.0883, -0.0167,\n",
      "         -0.0793, -0.1002, -0.0814,  0.1163, -0.0061,  0.0666,  0.0352,  0.1141,\n",
      "         -0.0987,  0.1165,  0.0634,  0.1051, -0.0530, -0.1000,  0.0592, -0.0206,\n",
      "         -0.0594, -0.0783, -0.0955, -0.0830, -0.0598, -0.0510,  0.1100, -0.0368,\n",
      "          0.0648, -0.1060,  0.0972,  0.1189,  0.0155,  0.0967, -0.1069, -0.0287,\n",
      "          0.0717,  0.0201, -0.0847, -0.0892,  0.1142,  0.0189,  0.1031,  0.1188,\n",
      "         -0.0357, -0.0372, -0.0756, -0.0415,  0.0366, -0.0126,  0.1061, -0.0678,\n",
      "         -0.0794, -0.1129, -0.0102, -0.0524,  0.0594, -0.0330, -0.0208, -0.0958],\n",
      "        [-0.0528, -0.0913,  0.0898, -0.0173, -0.1054,  0.0159,  0.0573, -0.0363,\n",
      "         -0.0953,  0.0136,  0.0595,  0.1087,  0.0872,  0.0586, -0.0974,  0.0298,\n",
      "          0.0779, -0.0791,  0.0656,  0.0258, -0.0246, -0.0919, -0.0016,  0.0219,\n",
      "         -0.0312, -0.0700, -0.0085,  0.1042,  0.0769, -0.0149,  0.1120, -0.0856,\n",
      "          0.1019,  0.0225,  0.0935, -0.1098, -0.0421,  0.0639,  0.0202, -0.1194,\n",
      "          0.0125,  0.0888,  0.0206,  0.1115,  0.0892, -0.0015,  0.0310, -0.0974,\n",
      "         -0.0190,  0.0266,  0.0270, -0.1216, -0.0848, -0.0140, -0.0687, -0.0333,\n",
      "         -0.0931,  0.0698,  0.1168, -0.0681,  0.0932,  0.0238, -0.0206, -0.0149],\n",
      "        [ 0.0895,  0.1032,  0.1110, -0.0576, -0.1147, -0.0086,  0.0759, -0.0320,\n",
      "          0.0962,  0.1154,  0.1022,  0.0312,  0.1164, -0.0922, -0.0028, -0.0580,\n",
      "          0.1200, -0.0716, -0.1135, -0.0246,  0.0269, -0.0833, -0.0170,  0.0107,\n",
      "         -0.0788, -0.0951,  0.0878, -0.1242, -0.0058, -0.0517,  0.0456,  0.0141,\n",
      "         -0.0635, -0.0538,  0.1170, -0.0211,  0.0948, -0.0619, -0.0082, -0.0653,\n",
      "          0.0193, -0.0521, -0.0482,  0.0727,  0.0038, -0.1137, -0.0804,  0.0414,\n",
      "         -0.0961,  0.0113,  0.0563, -0.0671, -0.0988,  0.0984, -0.1012, -0.0566,\n",
      "          0.0067,  0.0692,  0.0142, -0.0392, -0.0137, -0.0845,  0.1248,  0.0437],\n",
      "        [ 0.0626,  0.0704, -0.0716, -0.1193,  0.0198,  0.0364,  0.0541,  0.0180,\n",
      "          0.0040, -0.0529,  0.0256,  0.0752,  0.0717,  0.0460, -0.0194, -0.0286,\n",
      "         -0.0798,  0.0288,  0.0280, -0.0571, -0.0302,  0.1221, -0.0880, -0.0349,\n",
      "         -0.0504,  0.1004, -0.0737,  0.0018, -0.0568,  0.0712,  0.0131, -0.0718,\n",
      "         -0.0461, -0.0585, -0.0993, -0.0495,  0.1080,  0.0070, -0.1169, -0.0651,\n",
      "          0.0594, -0.0892, -0.0907,  0.0155, -0.0439, -0.0684,  0.0425,  0.0802,\n",
      "         -0.0462,  0.1008,  0.0599, -0.0507, -0.1093, -0.1213,  0.0308,  0.1164,\n",
      "         -0.0090, -0.0312, -0.0702,  0.0676,  0.0160, -0.0495,  0.0955,  0.0263],\n",
      "        [-0.0648, -0.1139, -0.0103, -0.0432, -0.1019, -0.0865,  0.0734, -0.0813,\n",
      "          0.0036,  0.0881,  0.0817, -0.0982, -0.0267,  0.0351,  0.1226,  0.0547,\n",
      "          0.0082,  0.0319,  0.0659, -0.0627, -0.0505,  0.0442, -0.0295,  0.0367,\n",
      "         -0.0178,  0.0131, -0.0902,  0.1101,  0.0287,  0.0535, -0.0166,  0.0578,\n",
      "         -0.0520,  0.0888, -0.1172,  0.0884,  0.1188, -0.0688, -0.1248,  0.0121,\n",
      "         -0.0204,  0.0770, -0.1196,  0.0016,  0.0510, -0.0733,  0.0218,  0.0528,\n",
      "         -0.0767,  0.0449, -0.0177, -0.1093, -0.0448, -0.0778,  0.0417,  0.0666,\n",
      "          0.0672,  0.1197, -0.0313,  0.0725,  0.0483,  0.0411, -0.0029,  0.0800],\n",
      "        [ 0.0467,  0.0608,  0.0242, -0.1086,  0.1180,  0.0774,  0.0138, -0.0574,\n",
      "         -0.0863,  0.0687,  0.0330, -0.0142,  0.0358,  0.0507,  0.0578,  0.0266,\n",
      "          0.1227, -0.0253, -0.0562,  0.0533, -0.0513, -0.0325,  0.0367, -0.0331,\n",
      "         -0.0686,  0.1066,  0.0595,  0.1083,  0.0634, -0.1233, -0.1055,  0.1053,\n",
      "         -0.0424, -0.0083,  0.0420,  0.1129, -0.1162,  0.0863,  0.1231,  0.0758,\n",
      "          0.1066, -0.0366,  0.0343, -0.0839, -0.0968,  0.1203, -0.0682, -0.0100,\n",
      "         -0.0305,  0.0421,  0.0573,  0.0908, -0.0066,  0.0028,  0.1008,  0.1168,\n",
      "         -0.0685, -0.1074, -0.0759,  0.0375, -0.0154, -0.0438,  0.0007,  0.0288]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1098, -0.0876,  0.0069,  0.0406,  0.0811,  0.0031, -0.0668, -0.0967,\n",
      "        -0.0639,  0.0213], device='cuda:0', requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A continuacion ingrese los parametros del modelo\n",
    "\n",
    "'''\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "model = VisionTransformer(\n",
    "    img_size=32,\n",
    "    patch_size=4,\n",
    "    embed_dim=64, \n",
    "    num_heads=2, \n",
    "    ff_dim=2, \n",
    "    num_layers=3,\n",
    "    num_classes=10, #Cifar10\n",
    "    dropout=0.1,\n",
    "    batch_first=True\n",
    ").to(device)\n",
    "\n",
    "\n",
    "print(\"Parametros del modelo:\", list(model.parameters()))\n",
    "\n",
    "# Definimos funciones de loss y optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "try:\n",
    "    model = torch.compile(model)\n",
    "except Exception as e:\n",
    "    print(\"Se produjo error durante la compilación:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de parámetros: 0.06 millones\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model: nn.Module) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of trainable parameters in a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of trainable parameters.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) / 1_000_000\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Número de parámetros: {:.2f} millones\".format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/98 [00:00<?, ?it/s]/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Quadro K620 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Quadro K620 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward /tmp/ipykernel_1641714/855988393.py line 66 \n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] due to: \n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 2234, in __call__\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                                ^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                      ^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                                                              ^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/scheduler.py\", line 3348, in create_backend\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] RuntimeError: Found Quadro K620 which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 5.0\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] \n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] \n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]           ^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] RuntimeError: Found Quadro K620 which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 5.0\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] \n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] \n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 2234, in __call__\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                                ^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                      ^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                                                              ^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/scheduler.py\", line 3348, in create_backend\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] RuntimeError: Found Quadro K620 which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 5.0\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] \n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] \n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]           ^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]   File \"/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] RuntimeError: Found Quadro K620 which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 5.0\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] \n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W1101 16:25:41.342000 1641714 torch/_dynamo/convert_frame.py:1125] \n",
      "/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Quadro K620 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "loss 2.30106: 100%|██████████| 98/98 [01:41<00:00,  1.04s/it]\n",
      "loss 2.30348: 100%|██████████| 98/98 [01:31<00:00,  1.08it/s]\n",
      "loss 2.31552: 100%|██████████| 98/98 [01:31<00:00,  1.08it/s]\n",
      "loss 2.29969: 100%|██████████| 98/98 [01:31<00:00,  1.07it/s]\n",
      "loss 2.30697: 100%|██████████| 98/98 [01:31<00:00,  1.07it/s]\n",
      "loss 2.30301: 100%|██████████| 98/98 [01:31<00:00,  1.07it/s]\n",
      "loss 2.30976: 100%|██████████| 98/98 [01:31<00:00,  1.07it/s]\n",
      "loss 2.30636: 100%|██████████| 98/98 [01:31<00:00,  1.07it/s]\n",
      "loss 2.30729: 100%|██████████| 98/98 [01:31<00:00,  1.07it/s]\n",
      "loss 2.30906: 100%|██████████| 98/98 [01:31<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, train_loader,test_loader,criterion,optimizer, device)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    trainer.train_model(use_amp=True)\n",
    "    \n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:07<00:00,  2.57it/s]\n",
      "/home/idk/git/vision_transformers_fiuba/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader):\n",
    "            images = images.to(device).float()\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Evaluar el modelo\n",
    "accuracy, precision, recall, f1 = evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.10\n",
      "Precision: 0.01\n",
      "Recall: 0.10\n",
      "F1 score: 0.02\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m images_so_far \u001b[38;5;241m==\u001b[39m num_images:\n\u001b[1;32m     23\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m visualize_classification(model, test_loader, \u001b[43mclass_names\u001b[49m, num_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     27\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_names' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def visualize_classification(model, data_loader, class_names, num_images=16):\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(tqdm(data_loader)):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images // 4, 4, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'True: {class_names[labels[j]]}\\nPred: {class_names[preds[j]]}')\n",
    "                img = inputs[j].cpu().numpy().transpose((1, 2, 0))\n",
    "                img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]  # Unnormalize\n",
    "                img = np.clip(img, 0, 1)\n",
    "                ax.imshow(img)\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    return\n",
    "\n",
    "\n",
    "visualize_classification(model, test_loader, class_names, num_images=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
